{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. dictionary with counts\n",
    "\n",
    "2. better way to deal with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lan = 'ru'\n",
    "ngram = 5\n",
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Load training data and change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/tunguz/ru-baseline-lb-0-9632\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../input/' + lan + '_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save n-gram dict from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2-gram\n"
     ]
    }
   ],
   "source": [
    "grouped = train.groupby('sentence_id')\n",
    "\n",
    "d = [{} for _ in range(ngram + 1)] # add 1 so d[n] for n-gram\n",
    "\n",
    "# For each dict\n",
    "for i in range(2, ngram + 1):\n",
    "    print('Working on ' + str(i) + '-gram')\n",
    "    \n",
    "    # For each sentence\n",
    "    for name, group in grouped:\n",
    "        before = group['before'].values.tolist()\n",
    "        after  = group['after'].values.tolist() \n",
    "        \n",
    "        # For tokens\n",
    "        for j in range(len(before) - i + 1):\n",
    "            key = [before[j + k] for k in range(i)]\n",
    "            value = [after[j + k] for k in range(i)]\n",
    "            d[i][tuple(key)] = value\n",
    "\n",
    "# d[2] # check 2-gram dict for debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save n-gram dict from external datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-00000-of-00100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "\n",
    "if debug_mode:\n",
    "    DATA_INPUT_PATH = r'../input/' + lan + '_with_types_test'\n",
    "else:\n",
    "    DATA_INPUT_PATH = r'../input/' + lan + '_with_types'\n",
    "    \n",
    "files = os.listdir(DATA_INPUT_PATH)\n",
    "\n",
    "for file in files:\n",
    "    train_ext = open(os.path.join(DATA_INPUT_PATH, file), encoding='UTF8')\n",
    "    print(file)\n",
    "    tmp = []\n",
    "    while 1:\n",
    "        line = train_ext.readline().strip()\n",
    "        \n",
    "        # Break the while loop when reaches the end\n",
    "        if line == '':\n",
    "            break\n",
    "\n",
    "        pos = line.find('\\t')\n",
    "        text = line[pos + 1:]\n",
    "        if text[:3] == '':\n",
    "            continue\n",
    "        arr = text.split('\\t')\n",
    "        \n",
    "        # Add line to tmp if not <eos> \n",
    "        if arr[0] != '<eos>':\n",
    "            before.append(arr[0])\n",
    "            after.append(arr[1])\n",
    "            \n",
    "        # Convert one sentence to dict    \n",
    "        else:    \n",
    "            \n",
    "            # For each dict\n",
    "            for i in range(2, ngram + 1):\n",
    "                # Modify 'after'\n",
    "                for j in range(len(before)):\n",
    "                    if after[j] == '<self>' or after[j] == 'sil':\n",
    "                        after[j] = before[j]\n",
    "                        \n",
    "                # For tokens\n",
    "                for j in range(len(before) - i + 1):\n",
    "                    key = [before[j + k] for k in range(i)]\n",
    "                    value = [after[j + k] for k in range(i)]\n",
    "                    \n",
    "                    if key != value: # Only save modified ones\n",
    "                        d[i][tuple(key)] = value            \n",
    "            \n",
    "            # Clear arrays　　\n",
    "            before = []   \n",
    "            after = []\n",
    "\n",
    "    train_ext.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Load test data and change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/' + lan + '_test_2.csv')\n",
    "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load bigdatatrick output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigdata = pd.read_csv('baseline_ext_en_2.csv')\n",
    "test['after'] = bigdata['after'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cover bigdatatrick output by n-gram results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2-gram\n"
     ]
    }
   ],
   "source": [
    "before = test['before'].values.tolist()\n",
    "token_id = test['token_id'].values.tolist()\n",
    "after = test['after'].values.tolist()\n",
    "\n",
    "for i in range(2, ngram + 1):\n",
    "    print('Working on ' + str(i) + '-gram')\n",
    "    \n",
    "    for j in range(0, len(test) - i + 1): \n",
    "        # n-grams to check\n",
    "        key = [before[j + k] for k in range(i)]\n",
    "        key = tuple(key)\n",
    "        \n",
    "        # Need to check token_id is 0 (tokens should be in on sentence)\n",
    "        if key in d[i] and (not 0 in token_id[j: j+i]):  \n",
    "            for k in range(i):\n",
    "                after[j + k] = d[i][key][k]\n",
    "\n",
    "    test['after'] = after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[['id','after']].to_csv('submission_' + str(ngram) + 'gram-bigdata_ext_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

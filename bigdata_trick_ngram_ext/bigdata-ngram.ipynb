{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. lower-case (will get worse)\n",
    "\n",
    "2. n-gram for external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lan = 'en'\n",
    "ngram = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Load training data and change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/tunguz/ru-baseline-lb-0-9632\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../input/' + lan + '_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save n-gram dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2-gram\n",
      "Working on 3-gram\n",
      "Working on 4-gram\n",
      "Working on 5-gram\n"
     ]
    }
   ],
   "source": [
    "grouped = train.groupby('sentence_id')\n",
    "\n",
    "d = [{} for _ in range(ngram + 1)] # add 1 so d[n] for n-gram\n",
    "\n",
    "# For each dict\n",
    "for i in range(2, ngram + 1):\n",
    "    print('Working on ' + str(i) + '-gram')\n",
    "    \n",
    "    # For each sentence\n",
    "    for name, group in grouped:\n",
    "        before = group['before'].values.tolist()\n",
    "        after  = group['after'].values.tolist() \n",
    "        \n",
    "        # For tokens\n",
    "        for j in range(len(before) - i + 1):\n",
    "            key = [before[j + k] for k in range(i)]\n",
    "            value = [after[j + k] for k in range(i)]\n",
    "            d[i][tuple(key)] = value\n",
    "\n",
    "# d[2] # check 2-gram dict for debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Load test data and change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/' + lan + '_test.csv')\n",
    "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load bigdatatrick output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigdata = pd.read_csv('baseline_ext_en.csv')\n",
    "test['after'] = bigdata['after'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cover bigdatatrick output by n-gram results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2-gram\n",
      "Working on 3-gram\n",
      "Working on 4-gram\n",
      "Working on 5-gram\n"
     ]
    }
   ],
   "source": [
    "before = test['before'].values.tolist()\n",
    "token_id = test['token_id'].values.tolist()\n",
    "after = test['after'].values.tolist()\n",
    "\n",
    "for i in range(2, ngram + 1):\n",
    "    print('Working on ' + str(i) + '-gram')\n",
    "    \n",
    "    for j in range(0, len(test) - i + 1): \n",
    "        # n-grams to check\n",
    "        key = [before[j + k] for k in range(i)]\n",
    "        key = tuple(key)\n",
    "        \n",
    "        # Need to check token_id is 0 (tokens should be in on sentence)\n",
    "        if key in d[i] and (not 0 in token_id[j: j+i]):  \n",
    "            for k in range(i):\n",
    "                after[j + k] = d[i][key][k]\n",
    "\n",
    "    test['after'] = after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[['id','after']].to_csv('submission_' + str(ngram) + 'gram-bigdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
